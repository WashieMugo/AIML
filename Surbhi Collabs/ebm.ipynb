{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b076bd1a-b236-4fbc-953d-8295b25122ae",
      "metadata": {
        "id": "b076bd1a-b236-4fbc-953d-8295b25122ae"
      },
      "source": [
        "# ⚡️ Energy-Based Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9235cbd1-f136-411c-88d9-f69f270c0b96",
      "metadata": {
        "id": "9235cbd1-f136-411c-88d9-f69f270c0b96"
      },
      "source": [
        "In this notebook, we'll walk through the steps required to train your own Energy Based Model to predict the distribution of a demo dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2531aef5-c81a-4b53-a344-4b979dd4eec5",
      "metadata": {
        "id": "2531aef5-c81a-4b53-a344-4b979dd4eec5"
      },
      "source": [
        "The code is adapted from the excellent ['Deep Energy-Based Generative Models' tutorial](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial8/Deep_Energy_Models.html) created by Phillip Lippe."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab_notebook_utils\n",
        "!pip install notebook-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2MGlVQgYY4p",
        "outputId": "617eb2c7-b15a-4845-cf49-826e5604ad1a"
      },
      "id": "m2MGlVQgYY4p",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: colab_notebook_utils in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Collecting notebook-utils\n",
            "  Downloading notebook_utils-0.2.0-py3-none-any.whl.metadata (538 bytes)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from notebook-utils) (0.44.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from notebook-utils) (1.13.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from notebook-utils) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from notebook-utils) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from notebook-utils) (3.7.1)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from notebook-utils) (1.2.14)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->notebook-utils) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->notebook-utils) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->notebook-utils) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->notebook-utils) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->notebook-utils) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->notebook-utils) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->notebook-utils) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->notebook-utils) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->notebook-utils) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->notebook-utils) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->notebook-utils) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->notebook-utils) (1.16.0)\n",
            "Downloading notebook_utils-0.2.0-py3-none-any.whl (3.9 kB)\n",
            "Installing collected packages: notebook-utils\n",
            "Successfully installed notebook-utils-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import notebook_utils\n",
        "dir(notebook_utils)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUqJbjoUZ-2C",
        "outputId": "98ea5ff1-68ee-4988-ff88-f2a55c684239"
      },
      "id": "EUqJbjoUZ-2C",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " 'os',\n",
              " 'sys']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "84acc7be-6764-4668-b2bb-178f63deeed3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84acc7be-6764-4668-b2bb-178f63deeed3",
        "outputId": "4fc87e40-5e6c-437d-93ab-e30ce743189b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import (\n",
        "    datasets,\n",
        "    layers,\n",
        "    models,\n",
        "    optimizers,\n",
        "    activations,\n",
        "    metrics,\n",
        "    callbacks,\n",
        ")\n",
        "\n",
        "# from notebook_utils import display, sample_batch\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5",
      "metadata": {
        "id": "339e6268-ebd7-4feb-86db-1fe7abccdbe5"
      },
      "source": [
        "## 0. Parameters <a name=\"parameters\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "1b2ee6ce-129f-4833-b0c5-fa567381c4e0",
      "metadata": {
        "id": "1b2ee6ce-129f-4833-b0c5-fa567381c4e0"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 32\n",
        "CHANNELS = 1\n",
        "STEP_SIZE = 10\n",
        "STEPS = 60\n",
        "NOISE = 0.005\n",
        "ALPHA = 0.1\n",
        "GRADIENT_CLIP = 0.03\n",
        "BATCH_SIZE = 128\n",
        "BUFFER_SIZE = 8192\n",
        "LEARNING_RATE = 0.0001\n",
        "EPOCHS = 120\n",
        "LOAD_MODEL = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "9a73e5a4-1638-411c-8d3c-29f823424458",
      "metadata": {
        "id": "9a73e5a4-1638-411c-8d3c-29f823424458"
      },
      "outputs": [],
      "source": [
        "# Load the data\n",
        "(x_train, _), (x_test, _) = datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "20697102-8c8d-4582-88d4-f8e2af84e060",
      "metadata": {
        "id": "20697102-8c8d-4582-88d4-f8e2af84e060"
      },
      "outputs": [],
      "source": [
        "# Preprocess the data\n",
        "\n",
        "\n",
        "def preprocess(imgs):\n",
        "    \"\"\"\n",
        "    Normalize and reshape the images\n",
        "    \"\"\"\n",
        "    imgs = (imgs.astype(\"float32\") - 127.5) / 127.5\n",
        "    imgs = np.pad(imgs, ((0, 0), (2, 2), (2, 2)), constant_values=-1.0)\n",
        "    imgs = np.expand_dims(imgs, -1)\n",
        "    return imgs\n",
        "\n",
        "\n",
        "x_train = preprocess(x_train)\n",
        "x_test = preprocess(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "13668819-2e42-4661-8682-33ff2c24ae8b",
      "metadata": {
        "id": "13668819-2e42-4661-8682-33ff2c24ae8b"
      },
      "outputs": [],
      "source": [
        "x_train = tf.data.Dataset.from_tensor_slices(x_train).batch(BATCH_SIZE)\n",
        "x_test = tf.data.Dataset.from_tensor_slices(x_test).batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "a7e1a420-699e-4869-8d10-3c049dbad030",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "a7e1a420-699e-4869-8d10-3c049dbad030",
        "outputId": "9887cea0-2a20-4a2d-c4b7-9f26483eaa54"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x200 with 10 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABJ4AAACmCAYAAACbdUU5AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAlKElEQVR4nO3dd3hUdfbH8RNa6EUSQKL0ZqgKoSii0pUiTUSlBFBZkKi7grrAQggiiAhSpAsu4goqXalKADWAIMIuYKGJwiJFmhAhQOb3x/48ngkTMil3Jpm8X8/D83xm5s6db3Iz7cv33BPkcrlcAgAAAAAAAGSwHP4eAAAAAAAAAAITE08AAAAAAABwBBNPAAAAAAAAcAQTTwAAAAAAAHAEE08AAAAAAABwBBNPAAAAAAAAcAQTTwAAAAAAAHAEE08AAAAAAABwBBNPAAAAAAAAcAQTTwAAIMsLCgqS6Ohofw/DUeXKlZO2bdv6exgAAACpwsQTAAAQEZH//Oc/0qVLFylbtqzkzZtXwsLCpEWLFjJlyhR/D83nypUrJ0FBQfovb968UrlyZRk8eLCcOXMmTfuMi4uT6OhoOXfuXMYOFgAAIBPL5e8BAAAA/4uLi5MHHnhAypQpI0899ZSUKlVKfv75Z9m6datMmjRJoqKi/D1En6tTp4688MILIiJy+fJl+frrr+XNN9+UTZs2yVdffZXq/cXFxcnIkSMlMjJSihYtmsGjBQAAyJyYeAIAADJ69GgpUqSIbN++/YZJkZMnT/pnUH4WFhYm3bt318tPPvmkFCxYUMaPHy/79++XypUr+3F0AAAAWQOldgAAQA4ePCjVq1f3uBKnRIkSbpfnzZsnTZs2lRIlSkhwcLCEh4fL9OnTb7jfH+ck2rhxo9SrV0/y5csnNWvWlI0bN4qIyJIlS6RmzZqSN29eqVu3rnzzzTdu94+MjJSCBQvKoUOHpFWrVlKgQAEpXbq0xMTEiMvlSvFnOnbsmPTp00dKliwpwcHBUr16dZk7d673vxQPSpUqJSIiuXL9+X93//73vyUyMlIqVKggefPmlVKlSkmfPn3k119/1W2io6Nl8ODBIiJSvnx5LeH78ccfdZsFCxZI/fr1JX/+/FKsWDFp0qSJrFu37oYxfPHFF1K/fn3JmzevVKhQQebPn5+unwkAAMBJrHgCAABStmxZ2bJli+zZs0dq1Khx022nT58u1atXl/bt20uuXLlk5cqVMmDAAElMTJRnnnnGbdsDBw7I448/Lv369ZPu3bvL+PHjpV27djJjxgwZMmSIDBgwQERExowZI127dpXvv/9ecuT48//Frl+/Lq1bt5aGDRvKuHHjZM2aNTJixAi5du2axMTEJDvGEydOSMOGDSUoKEgGDhwooaGhsnr1aunbt69cuHBBnn/++RR/J1evXpXTp0+LyP9K7b755huZMGGCNGnSRMqXL6/brV+/Xg4dOiS9e/eWUqVKyd69e2XWrFmyd+9e2bp1qwQFBUmnTp3khx9+kPfff18mTpwoISEhIiISGhoqIiIjR46U6OhoufvuuyUmJkby5Mkj27Ztkw0bNkjLli3dfp9dunSRvn37Sq9evWTu3LkSGRkpdevWlerVq6f4MwEAAPicCwAAZHvr1q1z5cyZ05UzZ05Xo0aNXC+++KJr7dq1roSEhBu2jY+Pv+G6Vq1auSpUqOB2XdmyZV0i4oqLi9Pr1q5d6xIRV758+VxHjhzR62fOnOkSEVdsbKxe16tXL5eIuKKiovS6xMREV5s2bVx58uRxnTp1Sq8XEdeIESP0ct++fV233nqr6/Tp025j6tatm6tIkSIefwZPY0/675577rlhn5729f7777tExLV582a97vXXX3eJiOvw4cNu2+7fv9+VI0cOV8eOHV3Xr193uy0xMfGGMdl9njx50hUcHOx64YUXbvrzAAAA+AuldgAAQFq0aCFbtmyR9u3by+7du2XcuHHSqlUrCQsLkxUrVrhtmy9fPs3nz5+X06dPy3333SeHDh2S8+fPu20bHh4ujRo10ssNGjQQEZGmTZtKmTJlbrj+0KFDN4xt4MCBmv9YwZSQkCCffvqpx5/F5XLJ4sWLpV27duJyueT06dP6r1WrVnL+/HnZuXNnir+TBg0ayPr162X9+vXy8ccfy+jRo2Xv3r3Svn17+f333z3+Pi5fviynT5+Whg0bioh49TjLli2TxMREGT58uNtqrz9+Xis8PFzuvfdevRwaGipVq1b1+HsDAADIDCi1AwAAIiISEREhS5YskYSEBNm9e7csXbpUJk6cKF26dJFdu3ZJeHi4iIh8+eWXMmLECNmyZYvEx8e77eP8+fNSpEgRvWwnl0REb7v99ts9Xn/27Fm363PkyCEVKlRwu65KlSoiIm7nR7JOnTol586dk1mzZsmsWbM8buPNCdNDQkKkefPmerlNmzZStWpV6dKli8yZM0c7/Z05c0ZGjhwpCxcuvGG/SSfiPDl48KDkyJFDf783k/T3KSJSrFixG35vAAAAmQUTTwAAwE2ePHkkIiJCIiIipEqVKtK7d2/58MMPZcSIEXLw4EFp1qyZVKtWTSZMmCC333675MmTR1atWiUTJ06UxMREt33lzJnT42Mkd73Li5OGp+SPMXTv3l169erlcZtatWqlad/NmjUTEZHNmzfrxFPXrl0lLi5OBg8eLHXq1JGCBQtKYmKitG7d+obfR3o5+XsDAABwAhNPAAAgWfXq1RMRkePHj4uIyMqVK+XKlSuyYsUKt9U3sbGxjjx+YmKiHDp0SFc5iYj88MMPIvK/rnmehIaGSqFCheT69etuK5YywrVr10RE5OLFiyLyvxVan332mYwcOVKGDx+u2+3fv/+G+yYtm/tDxYoVJTExUfbt2yd16tTJ0PECAAD4G+d4AgAAEhsb63HVzKpVq0REpGrVqiLy54obu+358+dl3rx5jo1t6tSpml0ul0ydOlVy586tq4+Sypkzp3Tu3FkWL14se/bsueH2U6dOpXksK1euFBGR2rVr62P9MS7rzTffvOG+BQoUEBGRc+fOuV3foUMHyZEjh8TExNywQoqVTAAAIKtjxRMAAJCoqCiJj4+Xjh07SrVq1SQhIUHi4uJk0aJFUq5cOendu7eIiLRs2VLy5Mkj7dq1k379+snFixdl9uzZUqJECV0VlZHy5s0ra9askV69ekmDBg1k9erV8sknn8iQIUMkNDQ02fuNHTtWYmNjpUGDBvLUU09JeHi4nDlzRnbu3CmffvqpnDlzJsXHPnbsmCxYsEBERM97NXPmTAkJCdEyu8KFC0uTJk1k3LhxcvXqVQkLC5N169bJ4cOHb9hf3bp1RURk6NCh0q1bN8mdO7e0a9dOKlWqJEOHDpVRo0bJvffeK506dZLg4GDZvn27lC5dWsaMGZOWXx0AAECmwMQTAACQ8ePHy4cffiirVq2SWbNmSUJCgpQpU0YGDBggw4YNk6JFi4rI/1Y+ffTRRzJs2DAZNGiQlCpVSvr37y+hoaHSp0+fDB9Xzpw5Zc2aNdK/f38ZPHiwFCpUSEaMGOFW1uZJyZIl5auvvpKYmBhZsmSJTJs2TYoXLy7Vq1eX1157zavH3rVrl/To0UNE/neS85CQEOnUqZOMGjVKwsLCdLt//etfEhUVJW+99Za4XC5p2bKlrF69WkqXLu22v4iICBk1apTMmDFD1qxZI4mJiXL48GEpUKCAxMTESPny5WXKlCkydOhQyZ8/v9SqVUsfHwAAIKsKcrGGGwAAZEKRkZHy0Ucf6fmUAAAAkPVwjicAAAAAAAA4goknAAAAAAAAOIKJJwAAAAAAADiCczwBAAAAAADAEax4AgAAAAAAgCOYeAIAAAAAAIAjmHgCAAAAAACAI5h4AgAAAAAAgCOYeAIAAAAAAIAjmHgCAAAAAACAI5h4AgAAAAAAgCOYeAIAAAAAAIAjmHgCAAAAAACAI5h4AgAAAAAAgCOYeAIAAAAAAIAjmHgCAAAAAACAI5h4AgAAAAAAgCOYeAIAAAAAAIAjmHgCAAAAAACAI5h4AgAAAAAAgCOYeAIAAAAAAIAjmHgCAAAAAACAI5h4AgAAAAAAgCOYeAIAAAAAAIAjmHgCAAAAAACAI5h4AgAAAAAAgCOYeAIAAAAAAIAjmHgCAAAAAACAI5h4AgAAAAAAgCOYeAIAAAAAAIAjmHgCAAAAAACAI5h4AgAAAAAAgCNyebthUFCQk+NAKrhcrgzbF8c18+C4BqaMPK4iHNvMhOdsYOK4BiaOa2DiPTZw8ZwNTBzXwOTNcWXFEwAAAAAAABzBxBMAAAAAAAAcwcQTAAAAAAAAHMHEEwAAAAAAABzBxBMAAAAAAAAcwcQTAAAAAAAAHMHEEwAAAAAAABzBxBMAAAAAAAAcwcQTAAAAAAAAHMHEEwAAAAAAABzBxBMAAAAAAAAcwcQTAAAAAAAAHMHEEwAAAAAAABzBxBMAAAAAAAAcwcQTAAAAAAAAHMHEEwAAAAAAABzBxBMAAAAAAAAcwcQTAAAAAAAAHJHL3wMAMlrdunU1Dxw4UHPPnj01z58/X/OUKVM079y50+HRAQAA+MakSZM0P/vss5r37Nnjtl3btm01HzlyxPmBAQB84rPPPtMcFBSkuWnTpj4dByueAAAAAAAA4AgmngAAAAAAAOCIgCy1y5kzp+YiRYp4dR9bkpU/f37NVatW1fzMM89oHj9+vObHHntM8+XLlzWPHTtW88iRI70aB1KvTp06bpfXr1+vuXDhwppdLpfmHj16aG7fvr3m4sWLOzBC+FuzZs00v/fee5rvu+8+zd9//71Px4TUGTZsmGb7epojx5//f3L//fdr3rRpk0/GBWQ3hQoV0lywYEHNbdq00RwaGqp5woQJmq9cueLw6CAiUq5cOc3du3fXnJiYqPmOO+5wu0+1atU0U2qXeVWpUkVz7ty5NTdp0kTztGnT3O5jj3tqLV++XHO3bt00JyQkpHmfuDl7XO+++27Nr776qtt299xzj8/GhKxn4sSJmu3fkT3djK+x4gkAAAAAAACOYOIJAAAAAAAAjsgSpXZlypTRnCdPHs122Vjjxo01Fy1aVHPnzp3T9dhHjx7VPHnyZM0dO3bU/Ntvv2nevXu3Zko9nFO/fn3NixcvdrvNllfa8jp7nOwSYVte17BhQ822w112XlJsl2/b39XSpUv9MZw0iYiI0Lx9+3Y/jgSpERkZqfmll17SnFzZgH2+A0g7W6pln3siIo0aNdJco0aNFPd16623arZd1eCcU6dOad68ebNme2oBZG7Vq1fXbN8LH3nkEc221Lx06dKak75Hpue90f7NzJgxQ/Pzzz+v+cKFC2neP25kv8fExsZq/uWXX9y2K1WqVLK3IXuyp/n5y1/+ovnq1auabYc7X2PFEwAAAAAAABzBxBMAAAAAAAAckWlL7Wynsg0bNmj2tktdetglqraT0sWLFzXbzljHjx/XfPbsWc10yUo/22Hwrrvu0rxgwQLNdhn/zezfv1/zuHHjNC9cuFDzl19+qdke+zFjxng54sBjO4VVrlxZc2YvtbNL0MuXL6+5bNmymoOCgnw6JqSOPVZ58+b140hgNWjQQLPtmGW7RIq4l4pYgwYN0vzf//5Xsy2Zt6/x27ZtS/tgcVO2k5ktnXniiSc058uXz+0+9nXz559/1mzL2W3HtK5du2q23ba+++67NI4aKbl06ZJmOtRlTfZz50MPPeTHkfypZ8+emt9++23N9rMznGNL65JeptQOIu6njLHdEb/44gvNH3zwgU/HZLHiCQAAAAAAAI5g4gkAAAAAAACOyLSldj/99JPmX3/9VXN6Su2SLtc/d+6c5gceeECz7WD27rvvpvnxkH4zZ87U/Nhjj6VrX7ZUr2DBgppt90FbVlarVq10PV6gsEurt2zZ4seRpI4twXzqqac02xIeSj0yn+bNm2uOioryuI09bm3bttV84sQJ5waWzT366KOaJ02apDkkJERz0tLVjRs3ag4NDdX8+uuve3wMe3+7fbdu3VI/YLixn51ee+01zfa4FipUyKt92bL1Vq1aabbL+u1z1P6N2Azn2O7OtWvX9t9AkGbr16/XnFyp3cmTJzXb0jd7qgGR5DvB2u7gSUulkflweoiszXYJHzp0qOak32/PnDmTqv3a+9tOswcPHtRsT3HgT6x4AgAAAAAAgCOYeAIAAAAAAIAjMm2pnV1mNnjwYM22rOKbb77RPHnyZI/72bVrl+YWLVq43Wa7ftjuO88991zqB4wMU7duXc1t2rTRnNwSU1sqJyKycuVKzePHj9dsuyfZvx3bibBp06YpPl52k3TJdlYxZ84cj9fbMhFkDrab2bx58zQnV1ptS7Xo2JSxcuX682NBvXr1NM+ePVuz7Ta6efNmzaNGjXLbl+2iEhwcrNl2VGnZsqXHcezYsSM1w0YKOnbsqPnJJ59M1X3tcn0R989StqtdpUqV0jg6ZDT7HC1TpoxX94mIiNBsSyV5jfWP6dOna162bJnHba5evao5LV3NChcurHnPnj2aS5cu7XF7Ow5eo33P5XK5Xabbb9Yya9YszbZLeHh4uNt29rOTN4YMGaK5ePHimu1pRnbv3p2qfTola36jBAAAAAAAQKbHxBMAAAAAAAAckWlL7Sy7tHPDhg2af/vtN822a0ffvn0121IrW1qX1N69ezU//fTTaR4r0qZOnTqabScPuwzYLjFdvXq15qTdAGxnjmHDhmm2pVenTp3SbJcf2s4ftszPdsTbuXPnTX6SrC9pN7+SJUv6aSTpk1yZlv37QubQq1cvzckt8bcd0ubPn+/0kLKt7t27a06uXNU+h2xXtAsXLiS7X7tdcuV1R48e1fzPf/4z5cHCa4888kiK2/z444+at2/frvmll15y286W11l33HFH2gaHDGdPLfDOO+9ojo6OTvY+9jbb9Xnq1KkZODJ469q1a5qTe86ll+1KWaxYsRS3t6/RV65ccWRM8J4th9+6dasfRwJvxMfHa7bfadNSMmm/N5ctW1az/R6bGUsxWfEEAAAAAAAARzDxBAAAAAAAAEdkiVI7K7ml/OfPn/d4vT2j+6JFi9xus8vR4HtVqlTRbDsX2hKp06dPaz5+/LhmW4Zx8eJFt/1+8sknHnNq5cuXT/MLL7yg+YknnkjzPrOChx56yO2y/T1kdrYssHz58h63OXbsmK+Gg2SEhIS4Xe7Tp49m+7psyz1eeeUVx8eVXdludLY7il0KPm3aNM22hPlm5XXW0KFDU9zm2Wef1WzLoZF+9rOQPZ3AunXrNB84cEDzyZMnU/0YWbUsO9DZ5/fNSu2QPXTr1k2zfV3w5rPe8OHDHRkT3NkyS/v9NukpJCpWrOizMSFt7OtvzZo1NX/77beave04V6BAAc22BN52MbUllx999FHqBusDrHgCAAAAAACAI5h4AgAAAAAAgCOyXKldcuzy4bp162q2Hc6aN2/udh+7xBy+ERwcrNl2HLTlXbZbYc+ePTXv2LFDs6/Lv8qUKePTx/OnqlWrJnub7f6YGdm/KVv28cMPP2i2f1/wnXLlymlevHixV/eZMmWK5tjY2IweUrZmSyZseV1CQoLmtWvXarbLun///XeP+0zaQcV2r7OvoUFBQZptCeXy5cu9GjtSz3Y5c6rcqlGjRo7sFxknR44//7+Z000ErqSnhHj55Zc1V6pUSXPu3LlT3NeuXbs0X716Nf2DQ4rsaQY+//xzzW3btvXDaJBat99+u2ZbzmpLKAcOHKjZ21MLTJgwQbPtVGvf3++5557UDdbHWPEEAAAAAAAARzDxBAAAAAAAAEcETKndpUuXNNtlbTt37tQ8e/Zst/vY0g1bxvXWW29ptl19kH533nmn5qTd0/7w8MMPa960aZPjY4L3tm/f7rfHLly4sObWrVtr7t69u2Zb2mPZrhJ2CTN8xx6zWrVqJbvdZ599pnnSpEmOjim7KVq0qOYBAwZotu9ztryuQ4cOKe7Tlm289957brfZsnfLdloZN25cio8B37MdBm0nnZuxHXusuLg4zVu2bEnfwJAutryOz7eZmy1P79Gjh+akpw3xpHHjxm6XvTnWtkOpLc1btWqV5uTKrIHsrkaNGpqXLl2q2XZxtqeP8Pb77aBBgzRHRkZ63Gb06NHeDtPvWPEEAAAAAAAARzDxBAAAAAAAAEcETKmddfDgQc12Wdq8efPctrNLV222y8rnz5+v+fjx4xk5zGzJnpHfdjaySw79WV5Hx5ebu+WWW1K1fe3atTXb422Xit92222a8+TJozlpVxZ7bOxy723btmm+cuWK5ly5/nx5+/rrr1M1bmQMW6o1duzYZLf74osvNPfq1Uvz+fPnHRlXdmWfX3b5t2VLrEqUKKG5d+/emtu3b6/ZLi8vWLCg275seYfNCxYs0GzL5OEb+fPn1xweHq55xIgRmpMrhRfx7n3SdtmxfzvXr19P3WCBbMS+nq5YsUKzLzor2+5ps2bNcvzxkH7Fixf39xCyHfvdwp7q4+2339ac3Huk7fz697//XbP9bizi/l3Ldq+z36Ps/MTMmTO9/wH8jBVPAAAAAAAAcAQTTwAAAAAAAHAEE08AAAAAAABwRECe48myLQ3379/vdputqWzWrJnmV199VXPZsmU123aFx44dy9BxBrK2bdtqrlOnjmZ7zg9by+5PybUa3rVrlx9G4x9J2+Xa38OMGTM0DxkyJMV91apVS7OtTb527Zrm+Ph4zfv27dM8d+5ct33t2LFDsz0P2IkTJzQfPXpUc758+TR/9913KY4VGcO2gF68eLFX9zl06JBmezyRsRISEjSfOnVKc2hoqObDhw9r9qYFtz2Xj23HLSJy6623aj59+rTmlStXejlipEfu3Lk133nnnZrt89IeI/vab4/rli1b3PbbunVrzfZ8UZY9D0anTp00T5o0SbP9ewTgzn5mstkb9hwzIt6ds9R+Vn/wwQc1r169OlWPDd+x51uEb3Tr1k3znDlzNNvPS/b5duDAAc316tXzmB9++GG3xwgLC9Ns36Pt57Y+ffqkeuyZASueAAAAAAAA4AgmngAAAAAAAOCIgC+1s/bs2eN2uWvXrprbtWuned68eZr79eunuXLlyppbtGjhxBADki15su28T548qXnRokU+HVNwcLDm6Ohoj9ts2LBBs217GegGDBjgdvnIkSOa77777lTt66efftK8bNkyzd9++63mrVu3pnKE7p5++mnNtmTIlm/Bd1566SXN3izvFxEZO3asU8OBce7cOc0dOnTQ/PHHH2u2bXwPHjyoefny5ZrfeecdzWfOnNG8cOFCt8ezS8ST3gZn2PdYWxK3ZMkSj9uPHDlSs33P+/LLLzXbv4mk29n275Z9LR4zZozm5N4Trly54nE/yFjJtflOqkmTJpqnTp3q6JjwJ/s95f7779ds27avXbtW8+XLl1P9GH379tUcFRWV6vvDt2JjYzXbckj4zqOPPqrZzhFcvXpVs/189fjjj2s+e/as5jfeeEPzfffdp9mW3Ym4l9baEr6QkBDNP//8s2b7WmE/t2VGrHgCAAAAAACAI5h4AgAAAAAAgCOyValdUnZZ3LvvvqvZnqXedmaxS4/tsraNGzc6Mr5AZ5fWHz9+3PHHs+V1w4YN0zx48GDNtiuaXRJ58eJFh0eXeb322mv+HsJN2Y6Ulrcd1ZB+tltly5YtU9zelm2JiHz//fcZPSSkYNu2bZptWVRq2fdFu3RcxL2Uh9JX59judbZ0zr63WbZL1ZQpUzTbz0T2b2LVqlVu969Zs6Zm25lu3Lhxmm0Jnu3Y895772n+9NNPNdv3GVuakFR26jDrhOQ69yZlOxGGh4drtp1n4Sx7mgPbVTu97OklKLXL/Gx5clL2td92Ybd/O0g/e9odezxeeeUVzbYELzn2+TZz5kzNjRo18moctgTPlmBm9vI6ixVPAAAAAAAAcAQTTwAAAAAAAHBEtiq1q1WrltvlLl26aI6IiNBsy+ssu8R48+bNGTy67GfFihWOP4YtAbJlB7ZDgS376dy5s+Njgm8sXbrU30PINtatW6e5WLFiHrex3QsjIyOdHhJ8xHYtTdoly5by0NUuY+XMmVPzqFGjNA8aNEjzpUuXNL/88sua7bGw5XW2s47tZHbnnXe6Pfb+/fs19+/fX7Nd+l+4cGHNthvqE088obl9+/aa169fL57Yzj0iIuXLl/e4HbwzY8YMzbZ85GZs59jnn38+o4cEH2vVqpW/h4BUuHbtWrK32dIrezoRZCz7PdF2iE36/pQS25UuuY6wIiKPPfaYZtvp0rKnhslKWPEEAAAAAAAARzDxBAAAAAAAAEcEZKld1apVNQ8cOFCz7dIhIlKqVKkU93X9+nXNtvNa0pICJM8uBbW5Q4cOmp977rkMe7y//vWvmv/xj39oLlKkiGbbWadnz54Z9thAdlS8eHHNyb02Tps2TXN27hIZaNauXevvIWRLtvzJltfFx8drtqVUthy2YcOGmnv37q35wQcf1GxLKGNiYtwe23bvSa7U4MKFC5rXrFnjMdtygscff9zjfuz7OdLvu+++8/cQIO7dyGwn2A0bNmj+/fffM+zx7PN80qRJGbZfOM+WeSV9/larVk2zLYMdMGCA4+PKTtLznLHfPR955BHNthw9aVe6Dz74IM2Pl9mx4gkAAAAAAACOYOIJAAAAAAAAjsjSpXa2VM4u2bbldeXKlUv1fnfs2KF59OjRmn3RhS0Q2c5GNtvjN3nyZM1z587V/Ouvv2q25QE9evTQXLt2bbfHu+222zT/9NNPmm1JiC37QeCwpZxVqlTRbDuqIWPYcpscOVL+P4y4uDgnhwM/oUOSfwwfPtzj9bbbne3kGh0drblSpUop7t9uP2bMGLfb7CkI0uP999/3mOGcKVOmaI6KinK7rWLFih7vY0+FYO+ftDwEN9e4cWPNQ4cO1dyiRQvNtmtjajtm3XLLLZofeught9smTJigOX/+/B7vb0v7Ll++nKrHhm/YkmkRkbCwMM1/+9vffD0ceMGWPdousCdPntTctGlTn47Jn1jxBAAAAAAAAEcw8QQAAAAAAABHZIlSu5IlS2oODw/XPHXqVM32zP7e2rZtm+bXX39ds+0gQPc659iSALsUsXPnzpptZ5zKlSt7tV9b0hMbG6s5udIEBA5byulN+RdSp06dOpqbN2+u2b5OJiQkaH7rrbc0nzhxwtnBwS8qVKjg7yFkS7/88ovm0NBQzcHBwZqTlqH/YdWqVZo3b96sedmyZZp//PFHzRlVWofMZe/evW6Xk3su8zk4Y9jvLDVq1PC4zYsvvqj5t99+S9X+bcneXXfd5Xab/Wxkbdy4UfP06dM128/OyLzscbWfveBfZcuW1fzkk09qtsdr1qxZmo8ePeqbgWUCfDMDAAAAAACAI5h4AgAAAAAAgCMyVamd7cgwc+ZMzba8I7XL+m3Z1RtvvOF2m+1yZrs5IGNt2bJF8/bt2zVHRER43N52u7Nllpbtdrdw4UK322wHFmRfjRo10vzOO+/4byABpGjRoprt89Q6duyY5kGDBjk9JPjZ559/rjlpeSslOs5p0qSJ5g4dOmi2JTa2a47tFnv27FnNlGdkX7bUQ0SkXbt2fhoJ/mC7XmUk+1qwcuVKzfbzMp3ssp7ChQtrfvjhhzUvXbrUH8PB/1u/fr1mW3a3YMECzSNGjPDpmDILVjwBAAAAAADAEUw8AQAAAAAAwBF+KbVr0KCB5sGDB2uuX7++5rCwsFTtMz4+XvPkyZM1v/rqq5ovXbqUqn0iY9iz9Xfq1Elzv379NA8bNizF/UyaNEmz7b5x4MCB9A4RASIoKMjfQwCylT179mjev3+/2222NL5ixYqaT5065fzAApztePXuu+96zMDN7Nu3z+3yt99+q/mOO+7w9XACXmRkpOaoqCjNvXr1SvM+Dx48qNl+D7Il0CLuZZX2NRtZS9euXd0uX7lyRbN9/sK/5s2bp3nUqFGaly9f7o/hZCqseAIAAAAAAIAjmHgCAAAAAACAI4JcLpfLqw0zsIRl7Nixmm2pXXLscuCPP/5Y87Vr1zTbjnXnzp1L5wgzNy8PmVcoTco8OK5pY5ev285Ns2fP1mzLOn0tI4+riH+Pre1kt2jRIs2NGzfWfPjwYc2VKlXyzcD8hOesO/tcFBGZM2eO5k2bNmm2ZSZJy30yA45rYOK4Bqas+B4bHBys2b5uvvLKK5qLFSumedmyZZptxyxbuvPLL79k8Cj9j+esu6RdvG1JbPv27TUfOXLEZ2NKC45rYPLmuLLiCQAAAAAAAI5g4gkAAAAAAACOYOIJAAAAAAAAjvDLOZ6QPtTGBiaOa2DKiuefgHd4zrorXLiw2+UPPvhAc/PmzTUvWbJEc+/evTVfunTJwdF5j+MamDiugYn32MDFczYwcVwDE+d4AgAAAAAAgN8w8QQAAAAAAABHUGqXBbFEMTBxXAMTZQCBi+fszdnSu9GjR2vu37+/5lq1amnet2+fbwaWAo5rYOK4BibeYwMXz9nAxHENTJTaAQAAAAAAwG+YeAIAAAAAAIAjKLXLgliiGJg4roGJMoDAxXM2MHFcAxPHNTDxHhu4eM4GJo5rYKLUDgAAAAAAAH7DxBMAAAAAAAAcwcQTAAAAAAAAHMHEEwAAAAAAABzBxBMAAAAAAAAcwcQTAAAAAAAAHMHEEwAAAAAAABzBxBMAAAAAAAAcEeRyuVz+HgQAAAAAAAACDyueAAAAAAAA4AgmngAAAAAAAOAIJp4AAAAAAADgCCaeAAAAAAAA4AgmngAAAAAAAOAIJp4AAAAAAADgCCaeAAAAAAAA4AgmngAAAAAAAOAIJp4AAAAAAADgiP8D4zjeMlpqbAMAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# # Show some items of clothing from the training set\n",
        "# train_sample = sample_batch(x_train)\n",
        "# display(train_sample)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def sample_batch(dataset, num_samples=10):\n",
        "    \"\"\"\n",
        "    Extract a sample batch of images from the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - dataset: the TensorFlow dataset to sample from.\n",
        "    - num_samples: number of images to sample.\n",
        "\n",
        "    Returns:\n",
        "    - A list of sampled images.\n",
        "    \"\"\"\n",
        "    return next(iter(dataset.take(1)))[0:num_samples]\n",
        "\n",
        "def display(images, title=\"Sample Batch\"):\n",
        "    \"\"\"\n",
        "    Display a grid of images.\n",
        "\n",
        "    Parameters:\n",
        "    - images: array or list of images to display.\n",
        "    - title: title for the plot.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, len(images), figsize=(15,2))\n",
        "    for i, image in enumerate(images):\n",
        "        axes[i].imshow(image.numpy().squeeze(), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "    plt.suptitle(title)\n",
        "    plt.show()\n",
        "\n",
        "# Now, sample and display the batch\n",
        "train_sample = sample_batch(x_train)\n",
        "display(train_sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f53945d9-b7c5-49d0-a356-bcf1d1e1798b",
      "metadata": {
        "id": "f53945d9-b7c5-49d0-a356-bcf1d1e1798b"
      },
      "source": [
        "## 2. Build the EBM network <a name=\"train\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "8936d951-3281-4424-9cce-59433976bf2f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "8936d951-3281-4424-9cce-59433976bf2f",
        "outputId": "6e9633c9-54c4-4b81-92cf-39871fc0d638"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m1\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m)          │             \u001b[38;5;34m416\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │           \u001b[38;5;34m4,640\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m18,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m36,928\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m16,448\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,640</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m76,993\u001b[0m (300.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">76,993</span> (300.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m76,993\u001b[0m (300.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">76,993</span> (300.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "ebm_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
        "x = layers.Conv2D(\n",
        "    16, kernel_size=5, strides=2, padding=\"same\", activation=activations.swish\n",
        ")(ebm_input)\n",
        "x = layers.Conv2D(\n",
        "    32, kernel_size=3, strides=2, padding=\"same\", activation=activations.swish\n",
        ")(x)\n",
        "x = layers.Conv2D(\n",
        "    64, kernel_size=3, strides=2, padding=\"same\", activation=activations.swish\n",
        ")(x)\n",
        "x = layers.Conv2D(\n",
        "    64, kernel_size=3, strides=2, padding=\"same\", activation=activations.swish\n",
        ")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(64, activation=activations.swish)(x)\n",
        "ebm_output = layers.Dense(1)(x)\n",
        "model = models.Model(ebm_input, ebm_output)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "32221908-8819-48fa-8e57-0dc5179ca2cf",
      "metadata": {
        "tags": [],
        "id": "32221908-8819-48fa-8e57-0dc5179ca2cf"
      },
      "outputs": [],
      "source": [
        "if LOAD_MODEL:\n",
        "    model.load_weights(\"./models/model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f392424-45a9-49cc-8ea0-c1bec9064d74",
      "metadata": {
        "id": "1f392424-45a9-49cc-8ea0-c1bec9064d74"
      },
      "source": [
        "## 2. Set up a Langevin sampler function <a name=\"sampler\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "bf10775a-0fbf-42df-aca5-be4b256a0c2b",
      "metadata": {
        "id": "bf10775a-0fbf-42df-aca5-be4b256a0c2b"
      },
      "outputs": [],
      "source": [
        "# Function to generate samples using Langevin Dynamics\n",
        "def generate_samples(\n",
        "    model, inp_imgs, steps, step_size, noise, return_img_per_step=False\n",
        "):\n",
        "    imgs_per_step = []\n",
        "    for _ in range(steps):\n",
        "        inp_imgs += tf.random.normal(inp_imgs.shape, mean=0, stddev=noise)\n",
        "        inp_imgs = tf.clip_by_value(inp_imgs, -1.0, 1.0)\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(inp_imgs)\n",
        "            out_score = model(inp_imgs)\n",
        "        grads = tape.gradient(out_score, inp_imgs)\n",
        "        grads = tf.clip_by_value(grads, -GRADIENT_CLIP, GRADIENT_CLIP)\n",
        "        inp_imgs += step_size * grads\n",
        "        inp_imgs = tf.clip_by_value(inp_imgs, -1.0, 1.0)\n",
        "        if return_img_per_step:\n",
        "            imgs_per_step.append(inp_imgs)\n",
        "    if return_img_per_step:\n",
        "        return tf.stack(imgs_per_step, axis=0)\n",
        "    else:\n",
        "        return inp_imgs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "180fb0a1-ed16-47c2-b326-ad66071cd6e2",
      "metadata": {
        "id": "180fb0a1-ed16-47c2-b326-ad66071cd6e2"
      },
      "source": [
        "## 3. Set up a buffer to store examples <a name=\"buffer\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "52615dcd-be2b-4e05-b729-0ec45ea6ef98",
      "metadata": {
        "id": "52615dcd-be2b-4e05-b729-0ec45ea6ef98"
      },
      "outputs": [],
      "source": [
        "class Buffer:\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.examples = [\n",
        "            tf.random.uniform(shape=(1, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)) * 2\n",
        "            - 1\n",
        "            for _ in range(BATCH_SIZE)\n",
        "        ]\n",
        "\n",
        "    def sample_new_exmps(self, steps, step_size, noise):\n",
        "        n_new = np.random.binomial(BATCH_SIZE, 0.05)\n",
        "        rand_imgs = (\n",
        "            tf.random.uniform((n_new, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)) * 2 - 1\n",
        "        )\n",
        "        old_imgs = tf.concat(\n",
        "            random.choices(self.examples, k=BATCH_SIZE - n_new), axis=0\n",
        "        )\n",
        "        inp_imgs = tf.concat([rand_imgs, old_imgs], axis=0)\n",
        "        inp_imgs = generate_samples(\n",
        "            self.model, inp_imgs, steps=steps, step_size=step_size, noise=noise\n",
        "        )\n",
        "        self.examples = tf.split(inp_imgs, BATCH_SIZE, axis=0) + self.examples\n",
        "        self.examples = self.examples[:BUFFER_SIZE]\n",
        "        return inp_imgs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "71a2a4a1-690e-4c94-b323-86f0e5b691d5",
      "metadata": {
        "id": "71a2a4a1-690e-4c94-b323-86f0e5b691d5"
      },
      "outputs": [],
      "source": [
        "class EBM(models.Model):\n",
        "    def __init__(self):\n",
        "        super(EBM, self).__init__()\n",
        "        self.model = model\n",
        "        self.buffer = Buffer(self.model)\n",
        "        self.alpha = ALPHA\n",
        "        self.loss_metric = metrics.Mean(name=\"loss\")\n",
        "        self.reg_loss_metric = metrics.Mean(name=\"reg\")\n",
        "        self.cdiv_loss_metric = metrics.Mean(name=\"cdiv\")\n",
        "        self.real_out_metric = metrics.Mean(name=\"real\")\n",
        "        self.fake_out_metric = metrics.Mean(name=\"fake\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [\n",
        "            self.loss_metric,\n",
        "            self.reg_loss_metric,\n",
        "            self.cdiv_loss_metric,\n",
        "            self.real_out_metric,\n",
        "            self.fake_out_metric,\n",
        "        ]\n",
        "\n",
        "    def train_step(self, real_imgs):\n",
        "        real_imgs += tf.random.normal(\n",
        "            shape=tf.shape(real_imgs), mean=0, stddev=NOISE\n",
        "        )\n",
        "        real_imgs = tf.clip_by_value(real_imgs, -1.0, 1.0)\n",
        "        fake_imgs = self.buffer.sample_new_exmps(\n",
        "            steps=STEPS, step_size=STEP_SIZE, noise=NOISE\n",
        "        )\n",
        "        inp_imgs = tf.concat([real_imgs, fake_imgs], axis=0)\n",
        "        with tf.GradientTape() as training_tape:\n",
        "            real_out, fake_out = tf.split(self.model(inp_imgs), 2, axis=0)\n",
        "            cdiv_loss = tf.reduce_mean(fake_out, axis=0) - tf.reduce_mean(\n",
        "                real_out, axis=0\n",
        "            )\n",
        "            reg_loss = self.alpha * tf.reduce_mean(\n",
        "                real_out**2 + fake_out**2, axis=0\n",
        "            )\n",
        "            loss = cdiv_loss + reg_loss\n",
        "        grads = training_tape.gradient(loss, self.model.trainable_variables)\n",
        "        self.optimizer.apply_gradients(\n",
        "            zip(grads, self.model.trainable_variables)\n",
        "        )\n",
        "        self.loss_metric.update_state(loss)\n",
        "        self.reg_loss_metric.update_state(reg_loss)\n",
        "        self.cdiv_loss_metric.update_state(cdiv_loss)\n",
        "        self.real_out_metric.update_state(tf.reduce_mean(real_out, axis=0))\n",
        "        self.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis=0))\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    def test_step(self, real_imgs):\n",
        "        batch_size = real_imgs.shape[0]\n",
        "        fake_imgs = (\n",
        "            tf.random.uniform((batch_size, IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
        "            * 2\n",
        "            - 1\n",
        "        )\n",
        "        inp_imgs = tf.concat([real_imgs, fake_imgs], axis=0)\n",
        "        real_out, fake_out = tf.split(self.model(inp_imgs), 2, axis=0)\n",
        "        cdiv = tf.reduce_mean(fake_out, axis=0) - tf.reduce_mean(\n",
        "            real_out, axis=0\n",
        "        )\n",
        "        self.cdiv_loss_metric.update_state(cdiv)\n",
        "        self.real_out_metric.update_state(tf.reduce_mean(real_out, axis=0))\n",
        "        self.fake_out_metric.update_state(tf.reduce_mean(fake_out, axis=0))\n",
        "        return {m.name: m.result() for m in self.metrics[2:]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "6337e801-eb59-4abe-84dc-9536cf4dc257",
      "metadata": {
        "id": "6337e801-eb59-4abe-84dc-9536cf4dc257"
      },
      "outputs": [],
      "source": [
        "ebm = EBM()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b14665-4359-447b-be58-3fd58ba69084",
      "metadata": {
        "id": "35b14665-4359-447b-be58-3fd58ba69084"
      },
      "source": [
        "## 3. Train the EBM network <a name=\"train\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "d9ec362d-41fa-473a-ad56-ebeec6cfd3b8",
      "metadata": {
        "id": "d9ec362d-41fa-473a-ad56-ebeec6cfd3b8"
      },
      "outputs": [],
      "source": [
        "# Compile and train the model\n",
        "ebm.compile(\n",
        "    optimizer=optimizers.Adam(learning_rate=LEARNING_RATE), run_eagerly=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "8ceca4de-f634-40ff-beb8-09ba42fd0f75",
      "metadata": {
        "id": "8ceca4de-f634-40ff-beb8-09ba42fd0f75"
      },
      "outputs": [],
      "source": [
        "tensorboard_callback = callbacks.TensorBoard(log_dir=\"./logs\")\n",
        "\n",
        "\n",
        "class ImageGenerator(callbacks.Callback):\n",
        "    def __init__(self, num_img):\n",
        "        self.num_img = num_img\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        start_imgs = (\n",
        "            np.random.uniform(\n",
        "                size=(self.num_img, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)\n",
        "            )\n",
        "            * 2\n",
        "            - 1\n",
        "        )\n",
        "        generated_images = generate_samples(\n",
        "            ebm.model,\n",
        "            start_imgs,\n",
        "            steps=1000,\n",
        "            step_size=STEP_SIZE,\n",
        "            noise=NOISE,\n",
        "            return_img_per_step=False,\n",
        "        )\n",
        "        generated_images = generated_images.numpy()\n",
        "        display(\n",
        "            generated_images,\n",
        "            save_to=\"./output/generated_img_%03d.png\" % (epoch),\n",
        "        )\n",
        "\n",
        "        example_images = tf.concat(\n",
        "            random.choices(ebm.buffer.examples, k=10), axis=0\n",
        "        )\n",
        "        example_images = example_images.numpy()\n",
        "        display(\n",
        "            example_images, save_to=\"./output/example_img_%03d.png\" % (epoch)\n",
        "        )\n",
        "\n",
        "\n",
        "image_generator_callback = ImageGenerator(num_img=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "627c1387-f29a-4cce-85a8-0903c1890e23",
      "metadata": {
        "id": "627c1387-f29a-4cce-85a8-0903c1890e23"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "class SaveModel(callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Ensure the models directory exists\n",
        "        if not os.path.exists(\"./models\"):\n",
        "            os.makedirs(\"./models\")\n",
        "\n",
        "        # Save the model weights\n",
        "        self.model.save_weights(\"./models/model.weights.h5\")\n",
        "\n",
        "        # Generate images (assuming this part is correct)\n",
        "        generated_images = self.model.generate_images()  # Replace this with your actual image generation code\n",
        "\n",
        "        # Save generated images manually\n",
        "        for i, img in enumerate(generated_images):\n",
        "            plt.imshow(img)  # Assuming img is in a format that can be displayed by plt\n",
        "            plt.axis('off')  # Turn off axis\n",
        "            plt.savefig(f\"./output/generated_img_{epoch:03d}_{i:02d}.png\", bbox_inches='tight', pad_inches=0)\n",
        "            plt.close()  # Close the figure to avoid memory issues\n",
        "\n",
        "# Instantiate the callback\n",
        "save_model_callback = SaveModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd6a5a71-eb55-4ec0-9c8c-cb11a382ff90",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bd6a5a71-eb55-4ec0-9c8c-cb11a382ff90",
        "outputId": "a018d7e2-2837-42cc-d483-9ec50521a4b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/120\n",
            "\u001b[1m408/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1:31\u001b[0m 2s/step - cdiv: -0.0022 - fake: 0.0025 - loss: -0.0012 - real: 0.0047 - reg: 0.0010"
          ]
        }
      ],
      "source": [
        "# Train the model with the callbacks\n",
        "ebm.fit(\n",
        "    x_train,\n",
        "    shuffle=True,\n",
        "    epochs=120,\n",
        "    validation_data=x_test,\n",
        "    callbacks=[\n",
        "        save_model_callback,\n",
        "        tensorboard_callback,\n",
        "        image_generator_callback,\n",
        "    ],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb1f295f-ade0-4040-a6a5-a7b428b08ebc",
      "metadata": {
        "id": "fb1f295f-ade0-4040-a6a5-a7b428b08ebc"
      },
      "source": [
        "## 4. Generate images <a name=\"generate\"></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8db3cfe3-339e-463d-8af5-fbd403385fca",
      "metadata": {
        "id": "8db3cfe3-339e-463d-8af5-fbd403385fca"
      },
      "outputs": [],
      "source": [
        "start_imgs = (\n",
        "    np.random.uniform(size=(10, IMAGE_SIZE, IMAGE_SIZE, CHANNELS)) * 2 - 1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80087297-3f47-4e0c-ac89-8758d4386d7c",
      "metadata": {
        "id": "80087297-3f47-4e0c-ac89-8758d4386d7c"
      },
      "outputs": [],
      "source": [
        "display(start_imgs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eaf4b749-5f6e-4a12-863f-b0bbcd23549c",
      "metadata": {
        "scrolled": true,
        "tags": [],
        "id": "eaf4b749-5f6e-4a12-863f-b0bbcd23549c"
      },
      "outputs": [],
      "source": [
        "gen_img = generate_samples(\n",
        "    ebm.model,\n",
        "    start_imgs,\n",
        "    steps=1000,\n",
        "    step_size=STEP_SIZE,\n",
        "    noise=NOISE,\n",
        "    return_img_per_step=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eac707f6-0597-499c-9a52-7cade6724795",
      "metadata": {
        "tags": [],
        "id": "eac707f6-0597-499c-9a52-7cade6724795"
      },
      "outputs": [],
      "source": [
        "display(gen_img[-1].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8476aaa1-e0e7-44dc-a1fd-cc30344b8dcb",
      "metadata": {
        "id": "8476aaa1-e0e7-44dc-a1fd-cc30344b8dcb"
      },
      "outputs": [],
      "source": [
        "imgs = []\n",
        "for i in [0, 1, 3, 5, 10, 30, 50, 100, 300, 999]:\n",
        "    imgs.append(gen_img[i].numpy()[6])\n",
        "\n",
        "display(np.array(imgs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e05d5a0-e124-40d1-80ed-552260c6e350",
      "metadata": {
        "id": "3e05d5a0-e124-40d1-80ed-552260c6e350"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}